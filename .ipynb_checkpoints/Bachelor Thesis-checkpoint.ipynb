{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hello!\n",
    "**This is my little machine.** <br>\n",
    "<font color=pink>Its task is to match CVs/resumes to job/project description according to the degree of suitability.</font> <br>\n",
    "<br>\n",
    "[Author: Michael Christian Suhendra](https://www.youtube.com/watch?v=dQw4w9WgXcQ \"Michael Christian Suhendra\") <br>\n",
    "# ðŸ˜˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prerequisites\n",
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data collection <br>\n",
    "\n",
    "The data generator reads a .csv file as mapping, containing the file name \n",
    "of the CV, the file name of the job post, and the corresponding label, \n",
    "and populates an array for both CV/job post pair and label respectively.\n",
    "\n",
    "Returns a numpy array with CV/job post pair (x) \n",
    "and a numpy array with labels (y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Hey tayo hey tayo dia bis kecil ramah' '']\n",
      " ['' '']\n",
      " ['' '']\n",
      " ['' '']\n",
      " ['' '']]\n",
      "['4' '5' '5' '5' '1']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "\n",
    "# reads a .txt file and returns the text body\n",
    "def read_file(folder, file):\n",
    "    data = ''\n",
    "    if os.path.isfile(os.path.join(folder, file + '.txt')):\n",
    "        filename = os.path.join(folder, file)\n",
    "        with open(filename + '.txt', 'r') as file:\n",
    "            data = file.read().replace('\\n', '')\n",
    "    return data\n",
    "    \n",
    "# create array with .csv file for mapping as input\n",
    "# TODO number of pairs with label n:\n",
    "# e.g. there will be more pairs with label 5 than 2\n",
    "# this transforms the entire input data into an array\n",
    "# should this be stored somewhere?\n",
    "def generate_data(filename):\n",
    "    pairs = []\n",
    "    labels = []\n",
    "    \n",
    "    with open(filename, 'rt') as csvDataFile:\n",
    "        csvReader = csv.reader(csvDataFile)\n",
    "        next(csvReader)\n",
    "        for row in csvReader:\n",
    "            cv_text = read_file('cv/', row[0])\n",
    "            jobpost_text = read_file('jobpost/', row[1])\n",
    "            \n",
    "            pairs += [[cv_text, jobpost_text]]\n",
    "            labels.append(row[2])\n",
    "    \n",
    "    return np.array(pairs), np.array(labels)\n",
    "\n",
    "pairs, labels = generate_data('data.csv')\n",
    "# print the first 5 pairs/labels\n",
    "print(pairs[:5])\n",
    "print(labels[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text preprocessing <br> \n",
    "This module gets raw text as input,\n",
    "'cleans' the text\n",
    "and returns the vector representation of words in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stop_words = set(stopwords.words('german'))\n",
    "germanStemmer = SnowballStemmer('german', ignore_stopwords = True)\n",
    "\n",
    "def cleanup_text(text):\n",
    "    # remove extra spaces\n",
    "    text = \" \".join(text.split())\n",
    "    # # remove punctuation\n",
    "    text = text.translate(str.maketrans('','',string.punctuation))\n",
    "    return text\n",
    "\n",
    "# Returns words as tokens (an array, with unique words as columns)\n",
    "def tokenize(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    for w in tokens:\n",
    "        w = lemmatize(w)\n",
    "    result = [i for i in tokens if not i in stop_words]\n",
    "    return result\n",
    "\n",
    "# Returns words in base form (stemming)\n",
    "def lemmatize(text):\n",
    "    text = germanStemmer.stem(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing train and test set <br>\n",
    "The entire prepared dataset is split into train and test data. To ensure a fair experiment, the sets are generated once and will be used for all models/algorithms below. However, when the program restarts, the entire dataset will be split differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# General machine learning functions: fit the model and compute F-score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The bag of words method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# initiate the vectorizer object\n",
    "cvec = CountVectorizer()\n",
    "\n",
    "# the text will be tokenized inside the CountVectorizer anyway.\n",
    "cvec.fit_transform\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed-forward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Flatten, Dense, Dropout, Lambda, Conv1D, MaxPooling1D\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import backend as K\n",
    "\n",
    "#TODO add softmax layer in the end with 5 categories\n",
    "def create_base_network(input_shape):\n",
    "    input = Input(shape=input_shape)\n",
    "    x = Flatten()(input)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    return Model(input, x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Siamese CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO maximum length from embedding layer\n",
    "def create_cnn_network(embedding_length):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(embedding_length, 1)))\n",
    "    model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    return model\n",
    "\n",
    "def contrastive_loss(y_true, y_pred):\n",
    "    '''Contrastive loss from Hadsell-et-al.'06\n",
    "    http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
    "    '''\n",
    "    margin = 1\n",
    "    sqaure_pred = K.square(y_pred)\n",
    "    margin_square = K.square(K.maximum(margin - y_pred, 0))\n",
    "    return K.mean(y_true * sqaure_pred + (1 - y_true) * margin_square)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the main module of the project. <br>\n",
    "The model can be chosen from the following:\n",
    "\n",
    "    - Bag-of-Words\n",
    "    - Tf-idf Vectorizer\n",
    "    - Doc2Vec/GloVe\n",
    "    - Feed-forward Neural Network\n",
    "    - Convolutional Neural Network\n",
    "    - Siamese Convolutional Neural Network\n",
    "    \n",
    "<br>\n",
    "TODO create the models in separate files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__():\n",
    "    print('init')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
