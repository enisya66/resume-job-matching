{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hello!\n",
    "**This is my little machine.** <br>\n",
    "<font color=pink>Its task is to match CVs/resumes to job/project description according to the degree of suitability.</font> <br>\n",
    "<br>\n",
    "[Author: Michael Christian Suhendra](https://www.youtube.com/watch?v=dQw4w9WgXcQ \"Michael Christian Suhendra\") <br>\n",
    "# üíØüî•üëåüèªüòÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prerequisites\n",
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data collection <br>\n",
    "\n",
    "The data generator reads a .csv file as mapping, containing the file name \n",
    "of the CV, the file name of the job post, and the corresponding label, \n",
    "and populates an array for both CV/job post pair and label respectively.\n",
    "\n",
    "Returns a numpy array with CV/job post pair (x) \n",
    "and a numpy array with labels (y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Hey tayo hey tayo dia bis kecil ramah' '']\n",
      " ['Karl Marx (* 5. Mai 1818 in Trier; √¢‚Ç¨\\xa0 14. M√É¬§rz 1883 in London) war ein deutscher Philosoph, √É‚Äìkonom, Gesellschaftstheoretiker, politischer Journalist, Protagonist der Arbeiterbewegung sowie Kritiker des Kapitalismus und der Religion.Zusammen mit Friedrich Engels wurde er zum einflussreichsten Theoretiker des Sozialismus und Kommunismus, deren Grundz√É¬ºge die beiden in der programmatischen Schrift Manifest der Kommunistischen Partei (1848) niederlegten. Als Marx√¢‚Ç¨‚Ñ¢ Hauptwerk gilt Das Kapital, dessen erster Band noch zu seinen Lebzeiten im Jahr 1867 erschien; die beiden folgenden B√É¬§nde wurden posthum von Engels herausgegeben. Einflussreich waren auch seine politischen Aktivit√É¬§ten in der entstehenden internationalen Arbeiterbewegung (Internationale Arbeiterassoziation), in der er zeitweise eine intellektuelle F√É¬ºhrungsrolle √É¬ºbernahm.Die theoretischen Grundlagen des nach Marx benannten Marxismus beeinflussen die Diskurse der Geschichtswissenschaft und Soziologie wie auch der Wirtschafts- und Politikwissenschaft bis in die Gegenwart.'\n",
      "  '']\n",
      " ['' '']\n",
      " ['' '']\n",
      " ['' '']]\n",
      "['4' '5' '5' '5' '1']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "\n",
    "# reads a .txt file and returns the text body\n",
    "def read_file(folder, file):\n",
    "    data = ''\n",
    "    if os.path.isfile(os.path.join(folder, file + '.txt')):\n",
    "        filename = os.path.join(folder, file)\n",
    "        with open(filename + '.txt', 'r') as file:\n",
    "            data = file.read().replace('\\n', '')\n",
    "    return data\n",
    "    \n",
    "# create array with .csv file for mapping as input\n",
    "# TODO number of pairs with label n:\n",
    "# e.g. there will be more pairs with label 5 than 2\n",
    "# this transforms the entire input data into an array\n",
    "# should this be stored somewhere?\n",
    "def generate_data(filename):\n",
    "    pairs = []\n",
    "    labels = []\n",
    "    \n",
    "    with open(filename, 'rt') as csvDataFile:\n",
    "        csvReader = csv.reader(csvDataFile)\n",
    "        next(csvReader)\n",
    "        for row in csvReader:\n",
    "            cv_text = read_file('cv/', row[0])\n",
    "            jobpost_text = read_file('jobpost/', row[1])\n",
    "            \n",
    "            pairs += [[cv_text, jobpost_text]]\n",
    "            labels.append(row[2])\n",
    "    \n",
    "    return np.array(pairs), np.array(labels)\n",
    "\n",
    "pairs, labels = generate_data('data.csv')\n",
    "# print the first 5 pairs/labels\n",
    "print(pairs[:5])\n",
    "print(labels[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text preprocessing <br> \n",
    "This module gets raw text as input,\n",
    "'cleans' the text\n",
    "and returns the vector representation of words in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['hey tayo hey tayo dia kecil ramah' '']\n",
      " ['karl marx 5 mai 1818 trier √¢‚Ç¨ 14 m√£¬§rz 1883 london deutsch philosoph √£‚Äìkonom gesellschaftstheoret polit journalist protagonist arbeiterbeweg sowi kritik kapitalismus religionzusamm friedrich engel wurd einflussreich theoret sozialismus kommunismus grundz√£¬ºg beid programmat schrift manif kommunist partei 1848 niederlegt marx√¢‚Ç¨‚Ñ¢ hauptwerk gilt kapital erst band lebzeit jahr 1867 erschi beid folgend b√£¬§nde wurd posthum engel herausgegeb einflussreich polit aktivit√£¬§t entsteh international arbeiterbeweg international arbeiterassoziation zeitweis intellektuell f√£¬ºhrungsroll √£¬ºbernahmdi theoret grundlag marx benannt marxismus beeinfluss diskurs geschichtswissenschaft soziologi wirtschaft politikwissenschaft gegenwart'\n",
      "  '']\n",
      " ['' '']\n",
      " ['' '']\n",
      " ['' '']]\n",
      "['4' '5' '5' '5' '1']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stop_words = set(stopwords.words('german'))\n",
    "germanStemmer = SnowballStemmer('german', ignore_stopwords = True)\n",
    "\n",
    "def cleanup_text(text):\n",
    "    # remove extra spaces\n",
    "    text = ' '.join(text.split())\n",
    "    # remove punctuation\n",
    "    text = text.translate(str.maketrans('','',string.punctuation))\n",
    "    # stemming\n",
    "    text = ' '.join(germanStemmer.stem(word) for word in text.split())\n",
    "    # remove stopwords\n",
    "    text = ' '.join(word for word in text.split() if word not in stop_words)\n",
    "    # lowercase\n",
    "    return text.lower()\n",
    "\n",
    "# Returns words as tokens (an array, with unique words as columns)\n",
    "def tokenize(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    for w in tokens:\n",
    "        w = lemmatize(w)\n",
    "    result = [i for i in tokens if not i in stop_words]\n",
    "    return result\n",
    "\n",
    "# Returns words in base form (stemming)\n",
    "def lemmatize(text):\n",
    "    text = germanStemmer.stem(text)\n",
    "    return text\n",
    "\n",
    "for p in pairs:\n",
    "    p[0] = cleanup_text(p[0])\n",
    "    p[1] = cleanup_text(p[1])\n",
    "    \n",
    "# print the first 5 pairs/labels\n",
    "print(pairs[:5])\n",
    "print(labels[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing train and test set <br>\n",
    "The entire prepared dataset is split into train and test data. To ensure a fair experiment, the sets are generated once and will be used for all models/algorithms below. In other words, the contents of the train and test data are the same for all models. <br> However, when the program restarts, the entire dataset will be split differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 15\n",
    "test_ratio = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(pairs, labels, test_size=test_ratio, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper methods for data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# copied from documentation\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    classes = classes[unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax\n",
    "\n",
    "def model_classification_report(y_true, y_pred, labels):\n",
    "    return classification_report(y_true, y_pred, labels=labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The bag of words method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-cb0f697d7a05>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# the text will be tokenized inside the CountVectorizer anyway.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcvec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpairs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;31m# print features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m    867\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    868\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[1;32m--> 869\u001b[1;33m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[0;32m    870\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    871\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m    790\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 792\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    793\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    794\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(doc)\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[1;32m--> 266\u001b[1;33m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[0;32m    267\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlowercase\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 232\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    233\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# initiate the vectorizer object\n",
    "cvec = CountVectorizer()\n",
    "\n",
    "# the text will be tokenized inside the CountVectorizer anyway.\n",
    "cv_vector_bow = cvec.fit_transform(x_train[:,0]).toarray()\n",
    "jobpost_vector_bow = cvec.fit_transform(x_train[:,1]).toarray()\n",
    "\n",
    "# classification algorithm: logistic regression\n",
    "# TODO end to end training\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed-forward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Flatten, Dense, Dropout, Lambda, Conv1D, MaxPooling1D\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import backend as K\n",
    "\n",
    "#TODO add softmax layer in the end with 5 categories\n",
    "def create_base_network(input_shape):\n",
    "    input = Input(shape=input_shape)\n",
    "    x = Flatten()(input)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    return Model(input, x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Siamese CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO maximum length from embedding layer\n",
    "def create_cnn_network(embedding_length):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(embedding_length, 1)))\n",
    "    model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    return model\n",
    "\n",
    "def contrastive_loss(y_true, y_pred):\n",
    "    '''Contrastive loss from Hadsell-et-al.'06\n",
    "    http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
    "    '''\n",
    "    margin = 1\n",
    "    sqaure_pred = K.square(y_pred)\n",
    "    margin_square = K.square(K.maximum(margin - y_pred, 0))\n",
    "    return K.mean(y_true * sqaure_pred + (1 - y_true) * margin_square)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the main module of the project. <br>\n",
    "The model can be chosen from the following:\n",
    "\n",
    "    - Bag-of-Words\n",
    "    - Tf-idf Vectorizer\n",
    "    - Doc2Vec/GloVe\n",
    "    - Feed-forward Neural Network\n",
    "    - Convolutional Neural Network\n",
    "    - Siamese Convolutional Neural Network\n",
    "    \n",
    "<br>\n",
    "TODO create the models in separate files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__():\n",
    "    print('init')\n",
    "\n",
    "# General machine learning functions: fit the model and compute F-score.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
